import tensorflow_hub as hub
import tensorflow as tf
import tensorflow_text as text  # Needed for loading universal-sentence-encoder-cmlm/multilingual-preprocess
import numpy as np

def normalization(embeds):
  norms = np.linalg.norm(embeds, 2, axis=1, keepdims=True)
  return embeds/norms

if _name_ == '_main_':

    english_sentences = tf.constant(["dog", "Puppies are nice.", "I enjoy taking long walks along the beach with my dog."])
    italian_sentences = tf.constant(["cane", "I cuccioli sono carini.", "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane."])
    japanese_sentences = tf.constant(["犬", "子犬はいいです", "私は犬と一緒にビーチを散歩するのが好きです"])
    arabic_sentences = tf.constant(["كلب", "الجراء لطيفة.", "أنا أستمتع بالمشي لمسافات طويلة على طول الشاطئ مع كلبي."])


    preprocessor = hub.KerasLayer(
        "https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2")
    encoder = hub.KerasLayer("https://tfhub.dev/google/LaBSE/2")

    english_embeds = encoder(preprocessor(english_sentences))["default"]
    japanese_embeds = encoder(preprocessor(japanese_sentences))["default"]
    italian_embeds = encoder(preprocessor(italian_sentences))["default"]
    arabic_embeds = encoder(preprocessor(arabic_sentences))["default"]
    # For semantic similarity tasks, apply l2 normalization to embeddings
    english_embeds = normalization(english_embeds)
    japanese_embeds = normalization(japanese_embeds)
    italian_embeds = normalization(italian_embeds)
    arabic_embeds = normalization(arabic_embeds)
    print (np.matmul(english_embeds, np.transpose(italian_embeds)))

    # English-Japanese similarity
    print (np.matmul(english_embeds, np.transpose(japanese_embeds)))

    # Italian-Japanese similarity
    print (np.matmul(italian_embeds, np.transpose(japanese_embeds)))
    #English-Arabic similarity
    print (np.matmul(english_embeds, np.transpose(arabic_embeds)))
